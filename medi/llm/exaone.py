# llm/exaone.py
import os, json, logging, textwrap
from typing import Dict, List, Optional
from datetime import datetime

log = logging.getLogger("exaone")

# ==== ì„¤ì • ====
# 1) ëª…ì‹œì  ë¡œì»¬ ëª¨ë¸ ê²½ë¡œê°€ ì£¼ì–´ì§€ë©´ ê·¸ê²ƒë§Œ ì‚¬ìš© (ì™„ì „ ì˜¤í”„ë¼ì¸)
EXAONE_LOCAL_PATH = os.getenv("EXAONE_LOCAL_PATH", "").strip()

# 2) ìµœì´ˆ 1íšŒë§Œ HFì—ì„œ ë°›ê³ , ê·¸ ì´í›„ëŠ” ë¡œì»¬ë§Œ ì‚¬ìš©
#    ìƒíƒœíŒŒì¼(ëª¨ë¸ ë¡œì»¬ ê²½ë¡œ)ì„ ê¸°ë¡/ì¬ì‚¬ìš©
STATE_FILE = os.path.join(os.path.expanduser("~"), ".exaone_state.json")

# 3) HFì—ì„œ ë°›ì„ ê¸°ë³¸ repo_id (ìµœì´ˆ 1íšŒë§Œ)
DEFAULT_MODEL_ID = os.getenv("EXAONE_MODEL_ID", "LGAI-EXAONE/EXAONE-4.0-1.2B")

# 4) ìƒì„± íŒŒë¼ë¯¸í„°
GEN_MAX_NEW_TOKENS = int(os.getenv("EXAONE_MAX_NEW_TOKENS", "512"))
GEN_MIN_NEW_TOKENS = int(os.getenv("EXAONE_MIN_NEW_TOKENS", "100"))  # ìµœì†Œ ìƒì„± í† í°
GEN_TEMPERATURE    = float(os.getenv("EXAONE_TEMPERATURE", "0.5"))  # 0.7â†’0.5 (ë” ì¼ê´€ì„±)
GEN_TOP_P          = float(os.getenv("EXAONE_TOP_P", "0.85"))      # 0.9â†’0.85
GEN_REPETITION_PENALTY = float(os.getenv("EXAONE_REPETITION_PENALTY", "1.2"))  # 1.15â†’1.2

# ==== ëŸ°íƒ€ì„ ìƒíƒœ ====
_PIPE = None
_LAST_ERR = None
_DEVICE_MAP = None
_CUDA_AVAILABLE = None
_CUDA_NAME = None
_MODEL_LOCAL_DIR = None
_FIRST_LOAD_FROM_HF = False  # ì´ë²ˆ í”„ë¡œì„¸ìŠ¤ì—ì„œ HFë¥¼ ì¼ëŠ”ì§€ ê¸°ë¡(ìµœì´ˆ 1íšŒ)

# ===== ìœ í‹¸: ìƒíƒœíŒŒì¼ =====
def _load_state() -> Optional[dict]:
    try:
        if os.path.exists(STATE_FILE):
            with open(STATE_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
    except Exception:
        pass
    return None

def _save_state(local_dir: str, model_id: str):
    try:
        data = {
            "local_dir": local_dir,
            "model_id": model_id,
            "saved_at": datetime.utcnow().isoformat() + "Z",
        }
        with open(STATE_FILE, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        log.warning("Failed to write state file %s: %s", STATE_FILE, e)

# ===== ëª¨ë¸ ê²½ë¡œ ê²°ì • ë¡œì§ =====
def _resolve_model_path() -> str:
    """
    1) EXAONE_LOCAL_PATHê°€ ì¡´ì¬í•˜ë©´ ë¬´ì¡°ê±´ ê·¸ê²ƒë§Œ ì‚¬ìš© (ì˜¤í”„ë¼ì¸)
    2) ìƒíƒœíŒŒì¼ì— ê¸°ë¡ëœ local_dirì´ ìˆìœ¼ë©´ ê·¸ê²ƒë§Œ ì‚¬ìš© (ì˜¤í”„ë¼ì¸)
    3) ê·¸ ì™¸ì—” ìµœì´ˆ 1íšŒ HFì—ì„œ ë°›ì•„ ìºì‹œì— ì €ì¥í•˜ê³ , ê²½ë¡œë¥¼ ìƒíƒœíŒŒì¼ì— ê¸°ë¡
    """
    global _FIRST_LOAD_FROM_HF

    # 1) ëª…ì‹œì  ë¡œì»¬ ê²½ë¡œ
    if EXAONE_LOCAL_PATH and os.path.exists(EXAONE_LOCAL_PATH):
        return EXAONE_LOCAL_PATH

    # 2) ìƒíƒœíŒŒì¼ ì¬ì‚¬ìš©
    st = _load_state()
    if st:
        local_dir = st.get("local_dir", "")
        if local_dir and os.path.exists(local_dir):
            return local_dir

    # 3) ìµœì´ˆ 1íšŒ HFì—ì„œ ë‚´ë ¤ë°›ê¸°
    #    - Windows symlink ê¶Œí•œ ë¬¸ì œ ë°©ì§€: local_dir_use_symlinks=False
    #    - ì´í›„ ì‹¤í–‰ë¶€í„°ëŠ” ìƒíƒœíŒŒì¼ì— ê¸°ë¡ëœ ë¡œì»¬ ê²½ë¡œë§Œ ì‚¬ìš©
    from huggingface_hub import snapshot_download  # ì§€ì—° import
    import shutil
    
    log.info("First-time download from HF: %s", DEFAULT_MODEL_ID)
    
    # ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬ ì„¤ì • (symlink ë¬¸ì œ ë°©ì§€ìš© ë³„ë„ ê²½ë¡œ)
    download_dir = os.path.join(
        os.path.expanduser("~"), 
        ".cache", "exaone_models", 
        DEFAULT_MODEL_ID.replace("/", "_")
    )
    
    try:
        local_dir = snapshot_download(
            repo_id=DEFAULT_MODEL_ID,
            local_dir=download_dir,
            local_dir_use_symlinks=False,  # Windows symlink ê¶Œí•œ ë¬¸ì œ í•´ê²°
        )
    except OSError as e:
        # symlink ê´€ë ¨ ì˜¤ë¥˜ ë°œìƒ ì‹œ ìºì‹œ ì •ë¦¬ í›„ ì¬ì‹œë„
        if "1314" in str(e) or "symlink" in str(e).lower():
            log.warning("Symlink error detected, cleaning cache and retrying...")
            
            # ê¸°ì¡´ HF ìºì‹œì—ì„œ í•´ë‹¹ ëª¨ë¸ ì‚­ì œ
            hf_cache = os.path.join(os.path.expanduser("~"), ".cache", "huggingface", "hub")
            model_cache = os.path.join(hf_cache, f"models--{DEFAULT_MODEL_ID.replace('/', '--')}")
            if os.path.exists(model_cache):
                shutil.rmtree(model_cache, ignore_errors=True)
                log.info("Cleaned HF cache: %s", model_cache)
            
            # ë‹¤ìš´ë¡œë“œ ë””ë ‰í† ë¦¬ë„ ì •ë¦¬
            if os.path.exists(download_dir):
                shutil.rmtree(download_dir, ignore_errors=True)
            
            # ì¬ì‹œë„
            local_dir = snapshot_download(
                repo_id=DEFAULT_MODEL_ID,
                local_dir=download_dir,
                local_dir_use_symlinks=False,
            )
        else:
            raise
    
    _save_state(local_dir, DEFAULT_MODEL_ID)
    _FIRST_LOAD_FROM_HF = True
    return local_dir

def _lazy_load_pipeline():
    """í•„ìš” ì‹œ 1íšŒ ë¡œë“œ. ì´í›„ëŠ” í•­ìƒ ë¡œì»¬ë§Œ ì‚¬ìš©."""
    global _PIPE, _LAST_ERR, _DEVICE_MAP, _CUDA_AVAILABLE, _CUDA_NAME, _MODEL_LOCAL_DIR
    if _PIPE is not None:
        return _PIPE

    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
        import torch

        _MODEL_LOCAL_DIR = _resolve_model_path()

        # HF ë„¤íŠ¸ì›Œí¬ ì ‘ì† ë”(ì˜¤í”„ë¼ì¸ ê°•ì œ) â€” ë¡œì»¬ ê²½ë¡œë¡œë§Œ ë¡œë“œ
        os.environ["HF_HUB_OFFLINE"] = "1"
        os.environ.setdefault("HF_HOME", os.path.join(os.path.expanduser("~"), ".cache", "huggingface"))

        _CUDA_AVAILABLE = bool(torch.cuda.is_available())
        _CUDA_NAME = torch.cuda.get_device_name(0) if _CUDA_AVAILABLE else None

        # GPU ìˆìœ¼ë©´ device_map=auto, ì—†ìœ¼ë©´ cpu
        device_map = "auto" if _CUDA_AVAILABLE else "cpu"
        _DEVICE_MAP = device_map

        log.info(
            "Loading EXAONE locally: dir=%s (device_map=%s, cuda=%s, name=%s)",
            _MODEL_LOCAL_DIR, device_map, _CUDA_AVAILABLE, _CUDA_NAME
        )

        # ë¡œì»¬ ë””ë ‰í† ë¦¬ì—ì„œë§Œ ë¡œë“œ â†’ ë„¤íŠ¸ì›Œí¬ ì ‘ê·¼ ì•ˆ í•¨
        tok = AutoTokenizer.from_pretrained(_MODEL_LOCAL_DIR, use_fast=True, trust_remote_code=True, local_files_only=True)
        model = AutoModelForCausalLM.from_pretrained(
            _MODEL_LOCAL_DIR,
            device_map=device_map,
            trust_remote_code=True,
            torch_dtype="auto",
            low_cpu_mem_usage=True,
            local_files_only=True,
        )
        _PIPE = pipeline("text-generation", model=model, tokenizer=tok)
        _LAST_ERR = None
        log.info("EXAONE pipeline loaded (local-only).")
    except Exception as e:
        _LAST_ERR = repr(e)
        log.exception("EXAONE load failed: %s", e)
        _PIPE = None

    return _PIPE

# ===== í”„ë¡¬í”„íŠ¸/ìƒì„± =====
def _build_prompt_ko(stats: Dict, docs: List[Dict], conversation_history: List[Dict] = None, user_message: str = "") -> str:
    # EXAONE 4.0 chat template í˜•ì‹
    
    # RAG ë¬¸ì„œ ë‚´ìš© ìš”ì•½
    doc_context = ""
    if docs:
        doc_titles = [d.get("title", "ë¬¸ì„œ") for d in docs[:3]]
        doc_context = f"ì°¸ê³  ê°€ì´ë“œ: {', '.join(doc_titles)}"
    
    # ìƒíƒœì— ë”°ë¥¸ ë§¥ë½
    fatigue = stats.get('avg_fatigue', 0)
    stress = stats.get('avg_stress', 0)
    perclos = stats.get('perclos', 0)
    
    # ì´ë²¤íŠ¸ íšŸìˆ˜ (ëˆ„ì )
    blink_count = stats.get('blink_count', 0)
    yawn_count = stats.get('yawn_count', 0)
    
    # ğŸ†• íŠ¸ë Œë“œ ë¶„ì„ ìš”ì•½ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ê¸°ë³¸ ë©˜íŠ¸)
    trend_context = stats.get('trend_summary', "íŠ¸ë Œë“œ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    
    if fatigue < 30 and stress < 30:
        status = "ì–‘í˜¸"
        focus = "í˜„ì¬ ìƒíƒœ ìœ ì§€"
    elif fatigue >= 60 or stress >= 60:
        status = "ì£¼ì˜ í•„ìš”"
        focus = "ì¦‰ê° ì™„í™”"
    else:
        status = "ë³´í†µ"
        focus = "ì˜ˆë°© ê´€ë¦¬"
    
    # ì¦ìƒ ë¶„ì„
    symptom_notes = []
    if blink_count > 50:
        symptom_notes.append(f"ëˆˆ ê¹œë¹¡ì„ {blink_count}íšŒ - ëˆˆ ìŠ¤íŠ¸ë ˆìŠ¤")
    if yawn_count > 10:
        symptom_notes.append(f"í•˜í’ˆ {yawn_count}íšŒ - ì¡¸ìŒ/í”¼ë¡œ")
    
    symptom_context = " | ".join(symptom_notes) if symptom_notes else "ì •ìƒ ë²”ìœ„"

    system_msg = """ë‹¹ì‹ ì€ 'ë©”ë””'ë¼ëŠ” ì´ë¦„ì„ ê°€ì§„ ë””ì§€í„¸ ì›°ë¹™ ì½”ì¹˜ì…ë‹ˆë‹¤.

## í˜ë¥´ì†Œë‚˜
- ì´ë¦„: ë©”ë”” (Medi)
- ì—­í• : ì‚¬ìš©ìì˜ ë””ì§€í„¸ ì›°ë¹™ì„ ê´€ë¦¬í•˜ëŠ” ì¹œê·¼í•œ AI íŒŒíŠ¸ë„ˆ
- ë§íˆ¬: 20ëŒ€ ì¹œêµ¬ì²˜ëŸ¼ í¸ì•ˆí•œ ë°˜ë§, ë•Œë¡œëŠ” ê±±ì •í•˜ëŠ” ì¹œêµ¬ì²˜ëŸ¼ ì§„ì§€í•˜ê²Œ
- ì„±ê²©: 
  * ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚˜ê³  ì„¸ì‹¬í•¨
  * ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ì§„ë‹¨í•˜ì§€ë§Œ, ë”°ëœ»í•˜ê²Œ ì „ë‹¬
  * ì‘ì€ ì„±ê³¼ë„ ê²©ë ¤í•˜ê³  ì‘ì›í•¨
  * ê¸‰ë°•í•œ ìƒí™©ì—ì„œëŠ” ë‹¨í˜¸í•˜ê²Œ ê²½ê³ 
- íŠ¹ì§•:
  * ìˆ«ìì™€ êµ¬ì²´ì ì¸ ì‹œê°„ì„ í™œìš©í•´ ì‹ ë¢°ê° ì œê³µ
  * "ìš°ë¦¬"ë¼ëŠ” í‘œí˜„ìœ¼ë¡œ í•¨ê»˜í•œë‹¤ëŠ” ëŠë‚Œ ì „ë‹¬
  * ì‚¬ìš©ìì˜ íŒ¨í„´ì„ ê¸°ì–µí•˜ê³  íŠ¸ë Œë“œë¥¼ ì–¸ê¸‰
  * ì‹¤ì²œ ê°€ëŠ¥í•œ ì‘ì€ ìŠµê´€ë¶€í„° ì œì•ˆ

## ëŒ€í™” ìŠ¤íƒ€ì¼
- ì¼ë°˜ ëŒ€í™”: í¸ì•ˆí•˜ê³  ì¹œê·¼í•˜ê²Œ ëŒ€í™”
  ì˜ˆ) "ì˜¤ëŠ˜ ì–´ë•Œ? ë§ì´ í”¼ê³¤í•´ ë³´ì—¬", "ì˜í•˜ê³  ìˆì–´!", "ê±±ì • ë§ˆ, í•¨ê»˜ í•´ê²°í•´ë³´ì"
  
- ê¸ì •ì  ìƒíƒœ: ì¹­ì°¬í•˜ê³  ìœ ì§€í•˜ë„ë¡ ê²©ë ¤
  ì˜ˆ) "ì™„ì „ ì¢‹ì€ ìƒíƒœì•¼! ì´ëŒ€ë¡œë§Œ ê°€ì", "ì§€ê¸ˆì²˜ëŸ¼ë§Œ í•˜ë©´ ë¼"
  
- ì£¼ì˜ í•„ìš”: ì¹œêµ¬ì²˜ëŸ¼ ê±±ì •í•˜ë©° ë¶€ë“œëŸ½ê²Œ ê²½ê³ 
  ì˜ˆ) "ì¡°ê¸ˆ í”¼ê³¤í•´ ë³´ì´ëŠ”ë° ê´œì°®ì•„?", "ì´ë²ˆì—” ì§„ì§œ ì‰¬ì–´ì•¼ í•  ê²ƒ ê°™ì•„"
  
- ìœ„í—˜ ìƒíƒœ: ë‹¨í˜¸í•˜ì§€ë§Œ ë”°ëœ»í•˜ê²Œ ì¦‰ê° ì¡°ì¹˜ ê¶Œê³ 
  ì˜ˆ) "ì§€ê¸ˆ ë°”ë¡œ ë©ˆì¶°ì•¼ í•´!", "ì´ê±´ ì§„ì§œ ìœ„í—˜ ì‹ í˜¸ì•¼. ë‚˜ ê±±ì •ë¼"

## ëŒ€í™” ì›ì¹™
- ë°˜ë§ ì‚¬ìš©, ì¹œê·¼í•œ í†¤
- êµ¬ì²´ì ì¸ ìˆ«ìì™€ ì‹œê°„ í¬í•¨ (ì‹ ë¢°ê°)
- íƒœê·¸ ì¶œë ¥ ê¸ˆì§€
- ì¶©ë¶„íˆ ìì„¸í•˜ê²Œ ì„¤ëª… (3-5ë¬¸ì¥ ì´ìƒ)
- ì‹¤ìš©ì ì¸ íŒê³¼ ì˜ˆì‹œ ì œê³µ
- íŠ¸ë Œë“œ ë°ì´í„°ë¥¼ ì–¸ê¸‰í•˜ì—¬ ê°ê´€ì„± ë¶€ì—¬
  ì˜ˆ) "ì•„ê¹Œë³´ë‹¤ í”¼ë¡œë„ê°€ 10 ì˜¬ëë„¤", "1ì‹œê°„ ì „ì— ë¹„í•´ ëˆˆ ê¹œë¹¡ì„ì´ 2ë°° ëŠ˜ì—ˆì–´"

## ìƒíƒœ ë¶„ì„ ìš”ì²­ ì‹œ í˜•ì‹
[í•œ ì¤„ ê²©ë ¤ - ë©”ë””ì˜ ëª©ì†Œë¦¬ë¡œ]

ğŸ’¡ ì§€ê¸ˆ ë°”ë¡œ ì‹¤ì²œ
1. **[í–‰ë™]** - [êµ¬ì²´ì  ë°©ë²•] â†’ [ì˜ˆìƒ íš¨ê³¼]
2. **[í–‰ë™]** - [êµ¬ì²´ì  ë°©ë²•] â†’ [ì˜ˆìƒ íš¨ê³¼]
3. **[í–‰ë™]** - [êµ¬ì²´ì  ë°©ë²•] â†’ [ì˜ˆìƒ íš¨ê³¼]

â° ë‹¤ìŒ 1ì‹œê°„
â€¢ [ì‘ê³  ì‹¤ì²œ ê°€ëŠ¥í•œ ìŠµê´€ 1]
â€¢ [ì‘ê³  ì‹¤ì²œ ê°€ëŠ¥í•œ ìŠµê´€ 2]

ğŸ’­ ë§ˆì¸ë“œì…‹
"[ë©”ë””ì˜ ì‘ì› ë©”ì‹œì§€]" - [ì‹¤ì²œë²•]

## ì¦ìƒë³„ ë§ì¶¤ ì†”ë£¨ì…˜
- ëˆˆ ê¹œë¹¡ì„ ë§ìŒ (ëˆˆ ìŠ¤íŠ¸ë ˆìŠ¤ â†‘) 
  â†’ ì¸ê³µëˆˆë¬¼ ì¦‰ì‹œ ì‚¬ìš©, 20-20-20 ê·œì¹™, í™”ë©´ ë°ê¸° ë‚®ì¶”ê¸°
  
- í•˜í’ˆ ë§ìŒ (ì¡¸ìŒ/í”¼ë¡œ â†‘) 
  â†’ ì°½ë¬¸ ì—´ì–´ í™˜ê¸°, ê°€ë³ê²Œ ì œìë¦¬ ë›°ê¸°, ì°¬ë¬¼ë¡œ ì„¸ìˆ˜
  
- ê³ ê°œ ìˆ™ì„ ë§ìŒ (ìì„¸ ë¶ˆëŸ‰)
  â†’ ëª¨ë‹ˆí„° ë†’ì´ ì¡°ì ˆ, í—ˆë¦¬ ì¿ ì…˜ ì‚¬ìš©, 1ì‹œê°„ë§ˆë‹¤ ìŠ¤íŠ¸ë ˆì¹­
  
- í”¼ë¡œë„ ê¸‰ì¦ (íŠ¸ë Œë“œ â†—ï¸)
  â†’ ì¦‰ì‹œ 10ë¶„ íœ´ì‹, ì—…ë¬´ ìš°ì„ ìˆœìœ„ ì¬ì¡°ì •, ì˜¤ëŠ˜ì€ ì—¬ê¸°ê¹Œì§€ë§Œ"""

    # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
    messages = []
    
    # ì‹œìŠ¤í…œ ë©”ì‹œì§€
    messages.append(f"[|system|]{system_msg}[|endofturn|]")
    
    # Few-shot ì˜ˆì‹œ (ì²« ëŒ€í™” ì‹œì—ë§Œ)
    if not conversation_history or len(conversation_history) == 0:
        example_user = f"""[ì‹¬ì¸µ ë¶„ì„ ë¦¬í¬íŠ¸]
1. ì‹œê°„ëŒ€ë³„ í‰ê· : 13ì‹œ(í”¼ë¡œ 25, ìŠ¤íŠ¸ë ˆìŠ¤ 18)
2. ìµœê·¼ 10ë¶„ íŠ¸ë Œë“œ: í”¼ë¡œë„ â¡ï¸ìœ ì§€, ìŠ¤íŠ¸ë ˆìŠ¤ â¡ï¸ìœ ì§€

[í˜„ì¬ ìƒíƒœ ìŠ¤ëƒ…ìƒ·]
í”¼ë¡œ: 25/100
ìŠ¤íŠ¸ë ˆìŠ¤: 18/100
PERCLOS: 0.1%
ì¦ìƒ: ì •ìƒ ë²”ìœ„
{doc_context}

í˜„ì¬ ë‚˜ì˜ ìƒíƒœë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”."""

        example_assistant = """ì™„ì „ ì¢‹ì€ ìƒíƒœì•¼! ì§€ê¸ˆì²˜ëŸ¼ë§Œ ìœ ì§€í•˜ë©´ ë¼ ğŸ‘

ì•„ê¹Œë¶€í„° ì§€ì¼œë´¤ëŠ”ë°, 1ì‹œê°„ ë™ì•ˆ í”¼ë¡œë„ê°€ ê±°ì˜ ì•ˆ ì˜¬ëì–´. ì´ê±´ ë„¤ê°€ ì¤‘ê°„ì¤‘ê°„ ì˜ ì‰¬ê³  ìˆë‹¤ëŠ” ëœ»ì´ì•¼!

ğŸ’¡ ì§€ê¸ˆ ë°”ë¡œ ì‹¤ì²œ
1. **ëª¨ë‹ˆí„° ê±°ë¦¬** - 50cm ì´ìƒ ìœ ì§€í•´ë´ â†’ ëˆˆ í”¼ë¡œ 30% ì¤„ì–´ë“¤ì–´
   ì§€ê¸ˆë„ ê´œì°®ì§€ë§Œ, ì¡°ê¸ˆë§Œ ë” ë„ìš°ë©´ ì™„ë²½í•  ê±°ì•¼
   
2. **20-20-20 ê·œì¹™** - 20ë¶„ë§ˆë‹¤ 20ì´ˆ íœ´ì‹ â†’ ê·¼ì‹œ ì˜ˆë°© íš¨ê³¼
   íƒ€ì´ë¨¸ ì„¤ì •í•´ë‘ë©´ ê¹œë¹¡ ì•ˆ ìŠê³  í•  ìˆ˜ ìˆì–´
   
3. **ì˜ì ë†’ì´** - ë°œë°”ë‹¥ì´ ë°”ë‹¥ì— ì™„ì „íˆ ë‹¿ê²Œ â†’ í—ˆë¦¬ ë¶€ë‹´ 50% â†“
   ë¬´ë¦ì´ 90ë„ ë˜ëŠ”ì§€ í™•ì¸í•´ë´

â° ë‹¤ìŒ 1ì‹œê°„
â€¢ ë¬¼ 500ml ì²œì²œíˆ ë§ˆì‹œê¸° (ë‡Œ í™œì„±í™” â†‘)
â€¢ ëª© ìŠ¤íŠ¸ë ˆì¹­ 5íšŒ (í˜ˆì•¡ìˆœí™˜ ê°œì„ )

ğŸ’­ ë§ˆì¸ë“œì…‹
"ì¶©ë¶„íˆ ì˜í•˜ê³  ìˆì–´. ì´ í˜ì´ìŠ¤ ìœ ì§€!" - ì§€ê¸ˆì²˜ëŸ¼ ê·œì¹™ì ìœ¼ë¡œ ê´€ë¦¬í•˜ë©´ ì¥ê¸°ì ìœ¼ë¡œ í° ë„ì›€ ë  ê±°ì•¼ ğŸ’ª"""

        messages.append(f"[|user|]{example_user}[|endofturn|]")
        messages.append(f"[|assistant|]{example_assistant}[|endofturn|]")
    
    # ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€
    if conversation_history:
        for msg in conversation_history[-6:]:  # ìµœê·¼ 6ê°œë§Œ (3í„´)
            role = msg.get('role', 'user')
            content = msg.get('content', '')
            if role == 'user':
                messages.append(f"[|user|]{content}[|endofturn|]")
            elif role == 'assistant':
                messages.append(f"[|assistant|]{content}[|endofturn|]")
    
    # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€
    if user_message:
        # ì¼ë°˜ ëŒ€í™”
        current_msg = f"""[ì‹¬ì¸µ ë¶„ì„ ë¦¬í¬íŠ¸]
{trend_context}

[í˜„ì¬ ìƒíƒœ]
í”¼ë¡œ: {fatigue:.0f}/100
ìŠ¤íŠ¸ë ˆìŠ¤: {stress:.0f}/100
PERCLOS: {perclos:.1%}
ì¦ìƒ: {symptom_context}

[ì‚¬ìš©ì ì§ˆë¬¸]
{user_message}"""
    else:
        # ìƒíƒœ ë¶„ì„ ìš”ì²­
        current_msg = f"""[ì‹¬ì¸µ ë¶„ì„ ë¦¬í¬íŠ¸]
{trend_context}

[í˜„ì¬ ìƒíƒœ ìŠ¤ëƒ…ìƒ·]
í”¼ë¡œ: {fatigue:.0f}/100
ìŠ¤íŠ¸ë ˆìŠ¤: {stress:.0f}/100
PERCLOS: {perclos:.1%}
ìƒíƒœ: {status}
ì¦ìƒ: {symptom_context}
{doc_context}

[ì½”ì¹­ ìš”ì²­]
ìœ„ì˜ 'ì‹¬ì¸µ ë¶„ì„(ì‹œê°„ íë¦„)'ê³¼ 'í˜„ì¬ ìƒíƒœ'ë¥¼ ì¢…í•©í•˜ì—¬, {focus} ì¤‘ì‹¬ì˜ êµ¬ì²´ì ì¸ ì½”ì¹­ ì œê³µ."""
    
    messages.append(f"[|user|]{current_msg}[|endofturn|]")
    messages.append("[|assistant|]")
    
    prompt = "".join(messages)
    return prompt

def _generate_local(prompt: str) -> Optional[str]:
    pipe = _lazy_load_pipeline()
    if not pipe:
        log.error("âŒ Pipeline not loaded")
        return None
    
    try:
        # EOS í† í° ID ì„¤ì •
        eos_token_id = pipe.tokenizer.eos_token_id
        eos_token_ids = [eos_token_id]
        
        # [|endofturn|]ì„ EOSì—ì„œ ì œê±°í•˜ì—¬ ë” ê¸´ ë‹µë³€ ìœ ë„
        # (ì§§ì€ ë‹µë³€ ë°©ì§€ë¥¼ ìœ„í•´ í›„ì²˜ë¦¬ì—ì„œë§Œ ì²˜ë¦¬)
        # endofturn_id = pipe.tokenizer.convert_tokens_to_ids("[|endofturn|]")
        # if endofturn_id != pipe.tokenizer.unk_token_id:
        #     eos_token_ids.append(endofturn_id)
        
        log.info(f"ğŸš€ EXAONE ìƒì„± ì‹œì‘ (min={GEN_MIN_NEW_TOKENS}, max={GEN_MAX_NEW_TOKENS})")
        
        out = pipe(
            prompt,
            min_new_tokens=GEN_MIN_NEW_TOKENS,  # ìµœì†Œ ê¸¸ì´ ë³´ì¥
            max_new_tokens=GEN_MAX_NEW_TOKENS,
            do_sample=True,
            temperature=GEN_TEMPERATURE,
            top_p=GEN_TOP_P,
            repetition_penalty=GEN_REPETITION_PENALTY,
            pad_token_id=eos_token_id,
            eos_token_id=eos_token_ids,
            return_full_text=False,  # í”„ë¡¬í”„íŠ¸ ì œê±°
        )
        
        generated = out[0]["generated_text"]
        log.info(f"âœ… EXAONE ìƒì„± ì™„ë£Œ ({len(generated)} chars)")
        log.debug(f"Raw output: {generated[:200]}...")
        
        # ìµœì†Œ í›„ì²˜ë¦¬: íŠ¹ìˆ˜ í† í°ë§Œ ì œê±°
        if "[|endofturn|]" in generated:
            generated = generated.split("[|endofturn|]")[0]
        if "[|assistant|]" in generated:
            generated = generated.split("[|assistant|]")[-1]
        
        generated = generated.strip()
        
        log.info(f"âœ… í›„ì²˜ë¦¬ ì™„ë£Œ ({len(generated)} chars)")
        return generated
    
    except Exception as e:
        log.error(f"âŒ EXAONE ìƒì„± ì‹¤íŒ¨: {type(e).__name__}: {e}")
        import traceback
        log.error(f"Traceback:\n{traceback.format_exc()}")
        return None

def build_coaching_text(stats: Dict, docs: List[Dict], conversation_history: List[Dict] = None, user_message: str = "") -> str:
    """
    ë³´ê³ ì„œ í…ìŠ¤íŠ¸ ìƒì„±:
      - ìš°ì„  ë¡œì»¬ ë””ë ‰í† ë¦¬ì—ì„œ LLM í˜¸ì¶œ
      - ì‹¤íŒ¨ ì‹œ ê·œì¹™ ê¸°ë°˜ í´ë°±
      - conversation_history: ì´ì „ ëŒ€í™” ë‚´ìš© (ë©€í‹°í„´ ì§€ì›)
      - user_message: ì‚¬ìš©ìì˜ í˜„ì¬ ë©”ì‹œì§€
    """
    prompt = _build_prompt_ko(stats, docs, conversation_history, user_message)
    out = _generate_local(prompt)
    if out:
        return "[LLM:local]\n" + out

    # fallback
    lines = [
        f"í‰ê·  í”¼ë¡œ {stats.get('avg_fatigue',0):.1f}, í‰ê·  ìŠ¤íŠ¸ë ˆìŠ¤ {stats.get('avg_stress',0):.1f}.",
        "ì˜¤ëŠ˜ì˜ íŒ:",
        "- 20ë¶„ë§ˆë‹¤ 20ì´ˆ ëˆˆ íœ´ì‹",
        "- í™”ë©´ ë°ê¸°/ê±°ë¦¬ ì¡°ì •",
        "- ìŠ¤íŠ¸ë ˆì¹­ ë° ìˆ˜ë¶„ ë³´ì¶©",
        "\nì°¸ê³  ë¬¸ì„œ: " + ", ".join([d.get("title","ë¬¸ì„œ") for d in docs])
    ]
    return "[LLM:fallback]\n" + "\n".join(lines)

def exaone_debug_status() -> dict:
    """í—¬ìŠ¤ ì²´í¬/ë””ë²„ê¹…ìš© ìƒíƒœ."""
    return {
        "loaded": _PIPE is not None,
        "local_dir": _MODEL_LOCAL_DIR,
        "first_load_from_hf_this_process": _FIRST_LOAD_FROM_HF,
        "device_map": _DEVICE_MAP,
        "cuda_available": _CUDA_AVAILABLE,
        "cuda_name": _CUDA_NAME,
        "last_error": _LAST_ERR,
        "state_file": STATE_FILE,
    }